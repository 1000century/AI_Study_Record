{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dbadb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "import yaml\n",
    "\n",
    "dotenv.load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92892f06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a32620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getenv(\"BASE_PATH\", default=\".\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", default=None)\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\", default=None)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key = OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "vectordb_path = \"C://Users/Sese/AI_Study_Record/RAG_AGENT/rag_0608/chroma_db\"\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"html_docs\",\n",
    "    persist_directory=vectordb_path,\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "vectorstore.delete_collection()\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"html_docs\",\n",
    "    persist_directory=vectordb_path,\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    api_key=GOOGLE_API_KEY,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1541f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import HTMLSectionSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "]\n",
    "\n",
    "\n",
    "html_splitter = HTMLSectionSplitter(\n",
    "    headers_to_split_on=headers_to_split_on,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d1684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "length ={}\n",
    "from tqdm import tqdm\n",
    "for path in os.listdir(base_path):\n",
    "    filename = os.path.join(base_path, path)\n",
    "    if int(path.split('_')[0])<=2508:\n",
    "        continue\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        html_content = file.read()\n",
    "    chunks = html_splitter.split_text(html_content)\n",
    "    # chunks = html_splitter.split_text_from_file(filename)\n",
    "    \n",
    "    title = ('_').join(path.split('_')[2:])[:-5]  # Assuming the file name ends with '.html'\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        doc = Document(\n",
    "            page_content=f\"ì œëª©: {title}\\n\\n{chunk.page_content}\",\n",
    "            metadata={\n",
    "                \"source\": filename,\n",
    "                \"chunk_metadata\": str(chunk.metadata),\n",
    "                \"chunk_index\": i,\n",
    "                \"total_chunks\": len(chunks),\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "        print(f\"{title} ({i+1}/{len(chunks)}) - {len(chunk.page_content)} characters\")\n",
    "        vectorstore.add_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e908073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sese\\AI_Study_Record\\RAG_AGENT\\rag_0608\n"
     ]
    }
   ],
   "source": [
    "vectorstore_path = r\"C:\\Users\\Sese\\AI_Study_Record\\RAG_AGENT\\rag_0608\\chroma_db\"\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"html_docs\",\n",
    "    persist_directory=vectorstore_path,\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "\n",
    "# ë²¡í„°ìŠ¤í† ì–´ì™€ retriever ì„¸íŒ…\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 10}\n",
    ")\n",
    "\n",
    "print(os.getcwd())\n",
    "with open(\"queries.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    queries = yaml.safe_load(f).get(\"queries\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6cf3dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Query: ëŽ…ê¸°ì—´ê³¼ ë¹„ìŠ·í•œ ë§¤ê°œì²´ë¡œ ì¸í•œ ê°ì—¼ë³‘ì€?\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\2522_5676_Leptospira interrogans.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\2522_5660_Vibrio spp..html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1757_4091_ìš”ë¡œê°ì—¼.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\2747_5639_ì—¼ì¦ì˜ ì–‘ìƒ.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\2521_5678_Pseudomonas aeruginosa.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\2525_5684_ëª¨ê¸° ë§¤ê°œ ê°ì—¼ë³‘.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1643_3848_ì—¬í–‰ ê´€ë ¨ ë°”ì´ëŸ¬ìŠ¤ ê°ì—¼.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1640_3835_ë…¹ë†ê·  ê°ì—¼.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1778_4160_ìœ„ì‹ë„ì—­ë¥˜ì§ˆí™˜.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1636_3826_ë°œì—´, ë¶ˆëª…ì—´.html\n",
      "\n",
      "ðŸ” Query: ë¶ˆëª…ì—´ ì§„ë‹¨ ìˆœì„œë¥¼ ì„¤ëª…í•˜ì‹œì˜¤\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1698_3968_ì„íšŒí™” ê±´ì—¼.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1698_3967_íšŒì „ê·¼ê°œíŒŒì—´.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1636_3826_ë°œì—´, ë¶ˆëª…ì—´.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\3442_5477_ê´€ì ˆì—¼ì˜ ê°œìš”.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1872_4389_ì§„ë‹¨ì„œ.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1799_4205_ìŠ¹ëª¨íŒ íƒˆì¶œ.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1922_4462_ë¦¼í”„ì¢….html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1643_3846_ì¤‘ì¦ì—´ì„±í˜ˆì†ŒíŒê°ì†Œì¦í›„êµ°.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1925_4500_ìœ ëŸ‰-ìš©ëŸ‰ ê³¡ì„ .html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1636_3826_ë°œì—´, ë¶ˆëª…ì—´.html\n",
      "\n",
      "ðŸ” Query: íŒ¨í˜ˆì¦ì˜ ì •ì˜ì™€ ì¹˜ë£Œ ì ‘ê·¼ë²•ì€?\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1636_4736_íŒ¨í˜ˆì¦.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1657_3890_ì €ì¹¼ìŠ˜í˜ˆì¦.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1912_4464_ë¹ˆí˜ˆì˜ ì ‘ê·¼.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1656_3900_ì´ìƒì§€ì§ˆí˜ˆì¦ì˜ ì¹˜ë£Œ.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1636_4736_íŒ¨í˜ˆì¦.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1654_3885_ë‹¹ë‡¨ì˜ ì¹˜ë£Œ.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1657_3889_ê³ ì¹¼ìŠ˜í˜ˆì¦.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1758_4067_ì €í˜ˆë‹¹.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1656_3900_ì´ìƒì§€ì§ˆí˜ˆì¦ì˜ ì¹˜ë£Œ.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1661_3905_ì „ì‹ ê²½í™”ì¦.html\n",
      "\n",
      "ðŸ” Query: ë°œì—´ì˜ ë³‘íƒœìƒë¦¬ ì„¤ëª…í•˜ì‹œì˜¤\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1654_3883_ë‹¹ë‡¨ì˜ ê°œìš”ì™€ ë³‘íƒœìƒë¦¬.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1868_4327_ë±€ë¬¼ë¦¼ê³¼ ë™ë¬¼ë¬¼ë¦¼.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1778_4161_ë§ë¡œë¦¬-ë°”ì´ìŠ¤ ì¦í›„êµ°.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1867_4342_í™”ìƒ.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1910_4458_ì¢…ì–‘ìš©í•´ì¦í›„êµ°.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\2752_5760_ë§ˆë¥´íŒ ì¦í›„êµ°.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1654_3883_ë‹¹ë‡¨ì˜ ê°œìš”ì™€ ë³‘íƒœìƒë¦¬.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1654_3883_ë‹¹ë‡¨ì˜ ê°œìš”ì™€ ë³‘íƒœìƒë¦¬.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1660_3913_í•­ì¸ì§€ì§ˆ ì¦í›„êµ°.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1740_4032_ë‹¤íƒœìž„ì‹ .html\n",
      "\n",
      "ðŸ” Query: Feverì˜ ë³‘íƒœìƒë¦¬ë¥¼ ì„¤ëª…í•˜ì‹œì˜¤.\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1636_3826_ë°œì—´, ë¶ˆëª…ì—´.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1643_3846_ì¤‘ì¦ì—´ì„±í˜ˆì†ŒíŒê°ì†Œì¦í›„êµ°.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1775_4110_ì‹ ìƒì•„ ì§ˆí™˜ - ê¸°íƒ€.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1636_3826_ë°œì—´, ë¶ˆëª…ì—´.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\2525_5699_SFTS virus.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1638_3853_Neutropenic fever.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\2525_5699_SFTS virus.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\2521_5659_Legionella pneumophila.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1638_3853_Neutropenic fever.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1641_3856_ë¼ìž„ë³‘, ìž¬ê·€ì—´.html\n",
      "\n",
      "ðŸ” Query: ì‹¬ë¶€ì „ìœ¼ë¡œ ìž…ì›í•œ í™˜ìžì˜ ì´ˆê¸° í‰ê°€ í•­ëª©ì€?\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1888_4435_ì •ì‹ ê³¼ì  í‰ê°€.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1862_4330_ì„ íƒì  ìˆ˜ìˆ ì— ëŒ€í•œ ë°˜ì‘.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1797_5451_ì‹¬í™” 1. ì‹¬ì´ˆìŒíŒŒ.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1801_4190_ì‹¬ê·¼ì—¼.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1672_3928_í—ˆí˜ˆì„± ë‡Œì¡¸ì¤‘.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1652_3895_ë¶€ì‹ ê¸°ëŠ¥ì €í•˜ì¦.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1648_3875_ë‚´ë¶„ë¹„ ì§ˆí™˜ì˜ ì ‘ê·¼.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1696_3954_ì™¸ìƒì˜ ì‘ê¸‰ì²˜ì¹˜.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1888_4435_ì •ì‹ ê³¼ì  í‰ê°€.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1800_4195_ì‹¬ë¶€ì „ ì›ì¸, ì¦ìƒ, ì§„ë‹¨.html\n",
      "\n",
      "ðŸ” Query: ì‹¬ë¶€ì „ìœ¼ë¡œ ìž…ì›í•œ í™˜ìžì—ê²Œ ì²˜ìŒ ì‹œí–‰í•´ì•¼ í•  ì²˜ì¹˜ëŠ”?\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1696_3953_ì‹¬ì •ì§€ í›„ í•©ë³‘ì¦.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1864_4329_ìˆ˜ìˆ  ì „ ì²˜ì¹˜.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\2440_4450_ì§€ì—­ì‚¬íšŒì •ì‹ ì˜í•™.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1759_4083_ì¤‘ì¶” ì‹ ê²½ê³„ì˜ ê°ì—¼ ì§ˆí™˜.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1804_4172_ê°ì—¼ ì‹¬ë‚´ë§‰ì—¼.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1882_4401_í—Œí˜ˆìž ê´€ë¦¬.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1647_3854_ì„±ì¸ì˜ ì˜ˆë°©ì ‘ì¢….html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1647_3854_ì„±ì¸ì˜ ì˜ˆë°©ì ‘ì¢….html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1636_4736_íŒ¨í˜ˆì¦.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1711_4015_ì„±í­í–‰.html\n",
      "\n",
      "ðŸ” Query: ê¸‰ì„± ì‹¬ë¶€ì „ìœ¼ë¡œ ìž…ì›í•œ í™˜ìžì—ê²Œ ì²˜ìŒ ì‹œí–‰í•´ì•¼ í•  ì²˜ì¹˜ëŠ”?\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1696_3953_ì‹¬ì •ì§€ í›„ í•©ë³‘ì¦.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1864_4329_ìˆ˜ìˆ  ì „ ì²˜ì¹˜.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\2440_4450_ì§€ì—­ì‚¬íšŒì •ì‹ ì˜í•™.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1711_4015_ì„±í­í–‰.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1789_4131_ê¸‰ì„± ê°„ë¶€ì „.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1868_4327_ë±€ë¬¼ë¦¼ê³¼ ë™ë¬¼ë¬¼ë¦¼.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1636_4736_íŒ¨í˜ˆì¦.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1864_4329_ìˆ˜ìˆ  ì „ ì²˜ì¹˜.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1804_4172_ê°ì—¼ ì‹¬ë‚´ë§‰ì—¼.html\n",
      "  - C:\\Users\\Sese\\autosave\\ì•Œë Œ ì´ë¡  ì¶”ì¶œ\\theory_texts\\1864_4329_ìˆ˜ìˆ  ì „ ì²˜ì¹˜.html\n",
      "âœ… query_log.md ìƒì„± ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "query_log = []\n",
    "\n",
    "for query in queries:\n",
    "    docs = retriever.invoke(query)\n",
    "    print(f\"\\nðŸ” Query: {query}\")\n",
    "\n",
    "    result_entry = {\"query\": query, \"results\": []}\n",
    "    \n",
    "    for doc in docs:\n",
    "        source = doc.metadata.get(\"source\", \"Unknown\")\n",
    "        excerpt = doc.page_content.replace(\"\\n\", \" \").strip()\n",
    "        result_entry[\"results\"].append({\n",
    "            \"source\": os.path.basename(source),\n",
    "            \"chunk_index\": doc.metadata.get(\"chunk_index\", 99),\n",
    "            \"total_chunks\": doc.metadata.get(\"total_chunks\", 99),\n",
    "            \"excerpt\": excerpt,\n",
    "        })\n",
    "        print(f\"  - {source}\")\n",
    "\n",
    "    query_log.append(result_entry)\n",
    "\n",
    "# ê²°ê³¼ ì €ìž¥\n",
    "with open(\"query_log.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.dump(query_log, f, allow_unicode=True)\n",
    "    \n",
    "    import yaml\n",
    "\n",
    "with open(\"query_log.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    log_data = yaml.safe_load(f)\n",
    "\n",
    "lines = [\"# ðŸ” RAG Query Log\\n\"]\n",
    "\n",
    "for entry in log_data:\n",
    "    query = entry.get(\"query\", \"\")\n",
    "    results = entry.get(\"results\", [])\n",
    "\n",
    "    lines.append(f\"\\n## ðŸ§ª Query: **{query}**\\n\")\n",
    "    lines.append(\"| Rank | Source | Excerpt |\")\n",
    "    lines.append(\"|------|--------|---------|\")\n",
    "\n",
    "    for i, result in enumerate(results, 1):\n",
    "        source = result.get(\"source\", \"N/A\")\n",
    "        excerpt = result.get(\"excerpt\", \"\").replace(\"|\", \"\\\\|\")  # Markdown-safe\n",
    "        lines.append(f\"| {i} | `{source}` | {excerpt} |\")\n",
    "\n",
    "# Markdown íŒŒì¼ ì €ìž¥\n",
    "with open(\"query_log.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(\"âœ… query_log.md ìƒì„± ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b34f060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# âš ï¸ ë³´ì•ˆ ì£¼ì˜: ì‹¤ì„œë¹„ìŠ¤ë‚˜ Git ì—…ë¡œë“œì—ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    api_key=GOOGLE_API_KEY,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19dc3633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are an expert in the field of medicine. Based on the following context, please answer the question.:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are an expert in the field of medicine. Based on the context provided below, analyze all relevant categories and provide a comprehensive answer. Do not limit your answer to only directly similar examplesâ€”include all entries that fall under the same classification or grouping where appropriate.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "ë‹¹ì‹ ì€ ì˜í•™ ë¶„ì•¼ì˜ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ì•„ëž˜ì— ì œê³µëœ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ, ê´€ë ¨ëœ ëª¨ë“  ë²”ì£¼ë¥¼ ë¶„ì„í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ì œì‹œí•˜ì‹­ì‹œì˜¤. ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ìœ ì‚¬í•œ ì˜ˆì—ë§Œ êµ­í•œí•˜ì§€ ë§ê³ , ê°™ì€ ë¶„ë¥˜ë‚˜ ë²”ì£¼ì— í•´ë‹¹í•˜ëŠ” í•­ëª©ë“¤ì„ ëª¨ë‘ í¬í•¨í•˜ë„ë¡ í•˜ì‹­ì‹œì˜¤.\n",
    "\n",
    "ë¬¸ë§¥:\n",
    "{context}\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "\n",
    "ë‹µë³€:\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1ccb73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_lines = [\"# LLM Query Results\\n\"]\n",
    "\n",
    "with open(\"query_log.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    log_data = yaml.safe_load(f)\n",
    "\n",
    "for entry in log_data:\n",
    "    readme_lines.append(f\"\\n## ðŸ§ª Query: **{entry['query']}**\\n\")\n",
    "    readme_lines.append(\"| Rank | Source | chunk | í‰ê°€ \")\n",
    "    readme_lines.append(\"|------|--------|---------|---------|\")\n",
    "    for i, result in enumerate(entry[\"results\"], 1):\n",
    "        source = result[\"source\"]\n",
    "        total_chunks = result[\"total_chunks\"]\n",
    "        chunk_index = result[\"chunk_index\"]\n",
    "        readme_lines.append(f\"| {i} | `{source}` | {chunk_index+1}/{total_chunks} |  |\")\n",
    "with open(\"README.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(readme_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93ce8249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Results saved\n"
     ]
    }
   ],
   "source": [
    "augmented_log = []\n",
    "markdown_lines = [\"# LLM Query Results\\n\"]\n",
    "\n",
    "with open(\"query_log.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    log_data = yaml.safe_load(f)\n",
    "\n",
    "for entry in log_data:\n",
    "    query = entry.get(\"query\", \"\")\n",
    "    results = entry.get(\"results\", [])\n",
    "    contents = [i['excerpt'] for i in results]\n",
    "    context = \"\\n\".join([f\"{'#'*10}\\n{i+1}. {content}\" for i, content in enumerate(contents)])\n",
    "    \n",
    "\n",
    "    llm_chain = prompt_template | llm\n",
    "    response = llm_chain.invoke({\"context\": context, \"question\": query}).content.strip()\n",
    "\n",
    "    # ë¡œê·¸ì— ì‘ë‹µ ì¶”ê°€\n",
    "    entry[\"llm_answer\"] = response\n",
    "    augmented_log.append(entry)\n",
    "\n",
    "    # Markdownì— í¬ë§· ì¶”ê°€\n",
    "    markdown_lines.append(f\"## ðŸ” Query\\n{query}\\n\")\n",
    "    markdown_lines.append(\"### ðŸ“š Context\\n\")\n",
    "    for idx, result in enumerate(results):\n",
    "        markdown_lines.append(f\"<details>\\n<summary>{idx+1}. {result['source']} ({result['chunk_index']+1}/{result['total_chunks']})</summary>\\n\")\n",
    "        markdown_lines.append(f\"{result['excerpt']}\\n\")\n",
    "        markdown_lines.append(\"</details>\\n\")\n",
    "    markdown_lines.append(f\"### ðŸ’¬ Answer\\n{response}\\n\")\n",
    "    markdown_lines.append(\"---\\n\")\n",
    "\n",
    "# YAMLë¡œ ì €ìž¥\n",
    "with open(\"[result] LLM_generated.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.dump(augmented_log, f, allow_unicode=True, sort_keys=False)\n",
    "\n",
    "# Markdownìœ¼ë¡œ ì €ìž¥\n",
    "with open(\"[result] LLM_generated.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(markdown_lines))\n",
    "\n",
    "print(\"âœ… Results saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
